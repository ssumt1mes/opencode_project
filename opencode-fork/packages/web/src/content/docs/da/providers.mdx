---
title: Leverandører
description: Bruge en hvilken som helst LLM-leverandør i OpenCode.
---

import config from "../../../../config.mjs"
export const console = config.console

OpenCode bruger [AI SDK](https://ai-sdk.dev/) og [Models.dev](https://models.dev) for at støtte **75+ LLM leverandører** og den støtter kjøring av lokale modeller.

For at tilføje til en leverandør må du:

1. Legg til API-nøklene for leverandøren ved at bruge kommandoen `/connect`.
2. Konfigurer leverandøren i OpenCode-konfigurasjonen.

---

### Legitimasjon

Når du tilføjer til en leverandørs API-nøkler med `/connect`-kommandoen, lagres de
i `~/.local/share/opencode/auth.json`.

---

### Konfig

Du kan tilpasse leverandørene gjennom `provider`-delen i OpenCode
konfig.

---

#### Base URL

Du kan tilpasse basen URL for enhver leverandør ved at angi alternativet `baseURL`. Dette er nyttig når du bruger proxy-tjenester eller tilpassede endepunkter.

```json title="opencode.json" {6}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "anthropic": {
      "options": {
        "baseURL": "https://api.anthropic.com/v1"
      }
    }
  }
}
```

---

## OpenCode Zen

OpenCode Zen er en liste over modeller levert av OpenCode-teamet som har vært
testet og verifisert for at fungere godt med OpenCode. [Finn ut mer](/docs/zen).

:::tip
Hvis du er ny, anbefaler vi at starte med OpenCode Zen.
:::

1. Kjør kommandoen `/connect` i TUI, velg opencode og gå til [opencode.ai/auth](https://opencode.ai/auth).

   ```txt
   /connect
   ```

2. Logg på, tilføj til faktureringsdetaljene dine og kopier API-nøgleen.

3. Lim inn API-nøgleen.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør `/models` i TUI for at se listen over modeller vi anbefaler.

   ```txt
   /models
   ```

Det fungerer som alle andre leverandører i OpenCode og er helt valgfritt at bruge.

---

## Katalog

La oss se på nogle av leverandørene i detalj. Hvis du vil tilføje til en leverandør til
liste, åpne gjerne en PR.

:::note
Ser du ikke en leverandør her? Send inn en PR.
:::

---

### 302.AI

1. Gå over til [302.AI-konsollen](https://302.ai/), opret en konto og generer en API-nøgle.

2. Kjør kommandoen `/connect` og søk etter **302.AI**.

   ```txt
   /connect
   ```

3. Skriv inn 302.AI API-nøgleen.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør kommandoen `/models` for at velge en modell.

   ```txt
   /models
   ```

---

### Amazonas grunnfjell

Slik bruger du Amazon Bedrock med OpenCode:

1. Gå over til **modellkatalogen** i Amazon Bedrock-konsollen og be om
   tilgang til modellene du ønsker.

   :::tip
   Du må ha tilgang til modellen du ønsker i Amazon Bedrock.
   :::

2. **Konfigurer autentisering** ved at bruge en av følgende metoder:

   #### Miljøvariabler (hurtigstart)

   Angi en av disse miljøvariablene mens du kjører opencode:

   ```bash
   # Option 1: Using AWS access keys
   AWS_ACCESS_KEY_ID=XXX AWS_SECRET_ACCESS_KEY=YYY opencode

   # Option 2: Using named AWS profile
   AWS_PROFILE=my-profile opencode

   # Option 3: Using Bedrock bearer token
   AWS_BEARER_TOKEN_BEDROCK=XXX opencode
   ```

   Eller tilføj dem til bash-profilen din:

   ```bash title="~/.bash_profile"
   export AWS_PROFILE=my-dev-profile
   export AWS_REGION=us-east-1
   ```

   #### Konfigurasjonsfil (anbefalt)

   For prosjektspesifikk eller vedvarende konfigurasjon, brug `opencode.json`:

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "amazon-bedrock": {
         "options": {
           "region": "us-east-1",
           "profile": "my-aws-profile"
         }
       }
     }
   }
   ```

   **Tilgjengelige alternativer:**
   - `region` - AWS region (f.eks. `us-east-1`, `eu-west-1`)
   - `profile` - AWS navngitt profil fra `~/.aws/credentials`
   - `endpoint` - Egendefinert endepunkt URL for VPC endepunkter (alias for generisk `baseURL`-alternativ)

   :::tip
   Alternativer for konfigurasjonsfil har forrang over miljøvariabler.
   :::

   #### Avansert: VPC Sluttpunkter

   Hvis du bruger VPC endepunkter for Berggrunn:

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "amazon-bedrock": {
         "options": {
           "region": "us-east-1",
           "profile": "production",
           "endpoint": "https://bedrock-runtime.us-east-1.vpce-xxxxx.amazonaws.com"
         }
       }
     }
   }
   ```

   :::note
   Alternativet `endpoint` er et alias for det generiske alternativet `baseURL`, ved at bruge AWS-spesifikk terminologi. Hvis både `endpoint` og `baseURL` er spesifisert, har `endpoint` forrang.
   :::

   #### Autentiseringsmetoder
   - **`AWS_ACCESS_KEY_ID` / `AWS_SECRET_ACCESS_KEY`**: Opret en IAM-bruger og generer tilgangsnøkler i AWS-konsollen
   - **`AWS_PROFILE`**: Brug navngitte profiler fra `~/.aws/credentials`. Konfigurer først med `aws configure --profile my-profile` eller `aws sso login`
   - **`AWS_BEARER_TOKEN_BEDROCK`**: Generer langsiktige API-nøkler fra Amazon Bedrock-konsollen
   - **`AWS_WEB_IDENTITY_TOKEN_FILE` / `AWS_ROLE_ARN`**: For EKS IRSA (IAM roller for tjenestekontoer) eller andre Kubernetes-miljøer med OIDC føderasjon. Disse miljøvariablene injiseres automatisk av Kubernetes når du bruger tjenestekontokommentarer.

   #### Autentiseringsprioritet

   Amazon Bedrock bruger følgende autentiseringsprioritet:
   1. **Bearer Token** - `AWS_BEARER_TOKEN_BEDROCK` miljøvariabel eller token fra kommandoen `/connect`
   2. **AWS legitimasjonskjede** - profil, tilgangsnøkler, delt legitimasjon, IAM roller, nettidentitetstokener (EKS IRSA), forekomstmetadata

   :::note
   Når et bærertoken er angitt (via `/connect` eller `AWS_BEARER_TOKEN_BEDROCK`), har det forrang over alle AWS legitimasjonsmetoder inkludert konfigurerte profiler.
   :::

3. Kjør kommandoen `/models` for at velge modellen du ønsker.

   ```txt
   /models
   ```

:::note
For egendefinerte slutningsprofiler, brug modellen og leverandørnavnet i nøkkelen og sett egenskapen `id` til arn. Dette sikrer korrekt caching:

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "amazon-bedrock": {
      // ...
      "models": {
        "anthropic-claude-sonnet-4.5": {
          "id": "arn:aws:bedrock:us-east-1:xxx:application-inference-profile/yyy"
        }
      }
    }
  }
}
```

:::

---

### Antropisk

1. Når du har registrert deg, kjør kommandoen `/connect` og velg Antropisk.

   ```txt
   /connect
   ```

2. Her kan du velge alternativet **Claude Pro/Max** og det vil åpne nettleseren din
   og ber deg om at autentisere.

   ```txt
   ┌ Select auth method
   │
   │ Claude Pro/Max
   │ Create an API Key
   │ Manually enter API Key
   └
   ```

3. Nå skal alle de antropiske modellene være tilgængelige når du bruger kommandoen `/models`.

   ```txt
   /models
   ```

:::info
Å bruge Claude Pro/Max-abonnementet ditt i OpenCode støttes ikke offisielt av [Anthropic](https://anthropic.com).
:::

##### Bruge API-tastene

Du kan også velge **Opret en API-nøgle** hvis du ikke har et Pro/Max-abonnement. Den åpner også nettleseren din og ber deg logge på Anthropic og gi deg en kode du kan lime inn i terminalen din.

Eller hvis du allerede har en API-nøgle, kan du velge **Angi API-nøgle manuelt** og lime den inn i terminalen.

---

### Azure OpenAI

:::note
Hvis du støter på «Beklager, men jeg kan ikke hjelpe med den forespørselen»-feil, kan du prøve at endre innholdsfilteret fra **DefaultV2** til **Default** i Azure-ressursen.
:::

1. Gå over til [Azure-portalen](https://portal.azure.com/) og lag en **Azure OpenAI**-ressurs. Du trenger:
   - **Ressursnavn**: Dette blir en del av API-endepunktet (`https://RESOURCE_NAME.openai.azure.com/`)
   - **API-nøgle**: Enten `KEY 1` eller `KEY 2` fra ressursen din

2. Gå til [Azure AI Foundry](https://ai.azure.com/) og distribuer en modell.

   :::note
   Distribusjonsnavnet må samsvare med modellnavnet for at opencode skal fungere skikkelig.
   :::

3. Kjør kommandoen `/connect` og søk etter **Azure**.

   ```txt
   /connect
   ```

4. Skriv inn API-nøgleen.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

5. Angi ressursnavnet ditt som en miljøvariabel:

   ```bash
   AZURE_RESOURCE_NAME=XXX opencode
   ```

   Eller tilføj den til bash-profilen din:

   ```bash title="~/.bash_profile"
   export AZURE_RESOURCE_NAME=XXX
   ```

6. Kjør kommandoen `/models` for at velge den distribuerte modellen.

   ```txt
   /models
   ```

---

### Azure Cognitive Services

1. Gå over til [Azure-portalen](https://portal.azure.com/) og lag en **Azure OpenAI**-ressurs. Du trenger:
   - **Ressursnavn**: Dette blir en del av API-endepunktet (`https://AZURE_COGNITIVE_SERVICES_RESOURCE_NAME.cognitiveservices.azure.com/`)
   - **API-nøgle**: Enten `KEY 1` eller `KEY 2` fra ressursen din

2. Gå til [Azure AI Foundry](https://ai.azure.com/) og distribuer en modell.

   :::note
   Distribusjonsnavnet må samsvare med modellnavnet for at opencode skal fungere skikkelig.
   :::

3. Kjør kommandoen `/connect` og søk etter **Azure Cognitive Services**.

   ```txt
   /connect
   ```

4. Skriv inn API-nøgleen.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

5. Angi ressursnavnet ditt som en miljøvariabel:

   ```bash
   AZURE_COGNITIVE_SERVICES_RESOURCE_NAME=XXX opencode
   ```

   Eller tilføj den til bash-profilen din:

   ```bash title="~/.bash_profile"
   export AZURE_COGNITIVE_SERVICES_RESOURCE_NAME=XXX
   ```

6. Kjør kommandoen `/models` for at velge den distribuerte modellen.

   ```txt
   /models
   ```

---

### Baseten

1. Gå over til [Baseten](https://app.baseten.co/), opret en konto og generer en API-nøgle.

2. Kjør kommandoen `/connect` og søk etter **Baseten**.

   ```txt
   /connect
   ```

3. Skriv inn din Baseten API-nøgle.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør kommandoen `/models` for at velge en modell.

   ```txt
   /models
   ```

---

### Cerebras

1. Gå over til [Cerebras-konsollen](https://inference.cerebras.ai/), opret en konto og generer en API-nøgle.

2. Kjør kommandoen `/connect` og søk etter **Cerebras**.

   ```txt
   /connect
   ```

3. Skriv inn Cerebras API-nøgleen.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør kommandoen `/models` for at velge en modell som _Qwen 3 Coder 480B_.

   ```txt
   /models
   ```

---

### Cloudflare AI Gateway

Cloudflare AI Gateway lar deg få tilgang til modeller fra OpenAI, Anthropic, Workers AI og mer gjennom et enhetlig endepunkt. Med [Unified Billing](https://developers.cloudflare.com/ai-gateway/features/unified-billing/) trenger du ikke separate API-nøkler for hver leverandør.

1. Gå over til [Cloudflare-dashbordet](https://dash.cloudflare.com/), naviger til **AI** > **AI Gateway**, og lag en ny gateway.

2. Angi konto ID og gateway ID som miljøvariabler.

   ```bash title="~/.bash_profile"
   export CLOUDFLARE_ACCOUNT_ID=your-32-character-account-id
   export CLOUDFLARE_GATEWAY_ID=your-gateway-id
   ```

3. Kjør kommandoen `/connect` og søk etter **Cloudflare AI Gateway**.

   ```txt
   /connect
   ```

4. Skriv inn Cloudflare API-tokenet ditt.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

   Eller angi den som en miljøvariabel.

   ```bash title="~/.bash_profile"
   export CLOUDFLARE_API_TOKEN=your-api-token
   ```

5. Kjør kommandoen `/models` for at velge en modell.

   ```txt
   /models
   ```

   Du kan også tilføje til modeller gjennom opencode-konfigurasjonen.

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "cloudflare-ai-gateway": {
         "models": {
           "openai/gpt-4o": {},
           "anthropic/claude-sonnet-4": {}
         }
       }
     }
   }
   ```

---

### Cortecs

1. Gå over til [Cortecs-konsollen](https://cortecs.ai/), opret en konto og generer en API-nøgle.

2. Kjør kommandoen `/connect` og søk etter **Cortecs**.

   ```txt
   /connect
   ```

3. Skriv inn Cortecs API-nøgleen.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør kommandoen `/models` for at velge en modell som _Kimi K2 Instruct_.

   ```txt
   /models
   ```

---

### DeepSeek

1. Gå over til [DeepSeek-konsollen](https://platform.deepseek.com/), opret en konto og klikk på **Opret ny API-nøgle**.

2. Kjør kommandoen `/connect` og søk etter **DeepSeek**.

   ```txt
   /connect
   ```

3. Skriv inn DeepSeek API-nøgleen.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør kommandoen `/models` for at velge en DeepSeek-modell som _DeepSeek Reasoner_.

   ```txt
   /models
   ```

---

### Deep Infra

1. Gå over til [Deep Infra-dashbordet](https://deepinfra.com/dash), opret en konto og generer en API-nøgle.

2. Kjør kommandoen `/connect` og søk etter **Deep Infra**.

   ```txt
   /connect
   ```

3. Skriv inn Deep Infra API-nøgleen.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør kommandoen `/models` for at velge en modell.

   ```txt
   /models
   ```

---

### Fastvare

1. Gå over til [Firmware dashboard](https://app.firmware.ai/signup), opret en konto og generer en API-nøgle.

2. Kjør kommandoen `/connect` og søk etter **Firmware**.

   ```txt
   /connect
   ```

3. Skriv inn firmware API-nøgleen.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør kommandoen `/models` for at velge en modell.

   ```txt
   /models
   ```

---

### Fyrverkeri AI

1. Gå over til [Fireworks AI-konsollen](https://app.fireworks.ai/), opret en konto og klikk på **Create API Key**.

2. Kjør kommandoen `/connect` og søk etter **Fireworks AI**.

   ```txt
   /connect
   ```

3. Skriv inn Fireworks AI API-nøgleen.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør kommandoen `/models` for at velge en modell som _Kimi K2 Instruct_.

   ```txt
   /models
   ```

---

### GitLab Duo

GitLab Duo gir AI-drevet agentchat med native verktøyoppringingsfunksjoner gjennom GitLabs antropiske proxy.

1. Kjør kommandoen `/connect` og velg GitLab.

   ```txt
   /connect
   ```

2. Velg autentiseringsmetoden din:

   ```txt
   ┌ Select auth method
   │
   │ OAuth (Recommended)
   │ Personal Access Token
   └
   ```

   #### Bruge OAuth (anbefalt)

   Velg **OAuth** og nettleseren din åpnes for autorisasjon.

   #### Bruger personlig tilgangstoken
   1. Gå til [GitLab User Settings > Access Tokens](https://gitlab.com/-/user_settings/personal_access_tokens)
   2. Klikk på **Legg til nytt token**
   3. Navn: `OpenCode`, omfang: `api`
   4. Kopier tokenet (starter med `glpat-`)
   5. Skriv den inn i terminalen

3. Kjør kommandoen `/models` for at se tilgængelige modeller.

   ```txt
   /models
   ```

   Tre Claude-baserte modeller er tilgængelige:
   - **duo-chat-haiku-4-5** (standard) - Raske svar for raske oppgaver
   - **duo-chat-sonnet-4-5** - Balansert ytelse for de fleste arbeidsflyter
   - **duo-chat-opus-4-5** - Mest egnet for kompleks analyse

:::note
Du kan også spesifisere 'GITLAB_TOKEN' miljøvariabel hvis du ikke vil
for at lagre token i opencode auth-lagring.
:::

##### Selvhostet GitLab

:::note[compliance note]
OpenCode bruger en liten modell for nogle AI oppgaver som at generere sessiontittelen.
Den er konfigurert til at bruge gpt-5-nano som standard, hostet av Zen. For at låse OpenCode
for kun at bruge din egen GitLab-hostede forekomst, tilføj til følgende i din
`opencode.json` fil. Det anbefales også at deaktivere sessiondeling.

```json
{
  "$schema": "https://opencode.ai/config.json",
  "small_model": "gitlab/duo-chat-haiku-4-5",
  "share": "disabled"
}
```

:::

For selvhostede GitLab-forekomster:

```bash
export GITLAB_INSTANCE_URL=https://gitlab.company.com
export GITLAB_TOKEN=glpat-...
```

Hvis forekomsten din kjører en brugerdefineret AI-gateway:

```bash
GITLAB_AI_GATEWAY_URL=https://ai-gateway.company.com
```

Eller tilføj til bash-profilen din:

```bash title="~/.bash_profile"
export GITLAB_INSTANCE_URL=https://gitlab.company.com
export GITLAB_AI_GATEWAY_URL=https://ai-gateway.company.com
export GITLAB_TOKEN=glpat-...
```

:::note
GitLab-administratoren din må aktivere følgende:

1. [Duo Agent Platform](https://docs.gitlab.com/user/gitlab_duo/turn_on_off/) for brugeren, gruppen eller forekomsten
2. Funksjonsflagg (via Rails-konsollen):
   - `agent_platform_claude_code`
   - `third_party_agents_enabled`
     :::

##### OAuth for selvvertsbaserte forekomster

For at få Oauth til at fungere for din selvhostede forekomst, må du oprete
en ny applikasjon (Innstillinger → Programmer) med
callback URL `http://127.0.0.1:8080/callback` og følgende omfang:

- api (Få tilgang til API på dine vegne)
- read_user (Les din personlige informasjon)
- read_repository (tillater skrivebeskyttet tilgang til depotet)

Utsett deretter applikasjonen ID som miljøvariabel:

```bash
export GITLAB_OAUTH_CLIENT_ID=your_application_id_here
```

Mer dokumentasjon på [opencode-gitlab-auth](https://www.npmjs.com/package/@gitlab/opencode-gitlab-auth) hjemmeside.

##### Konfigurasjon

Tilpass gjennom `opencode.json`:

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "gitlab": {
      "options": {
        "instanceUrl": "https://gitlab.com",
        "featureFlags": {
          "duo_agent_platform_agentic_chat": true,
          "duo_agent_platform": true
        }
      }
    }
  }
}
```

##### GitLab API Verktøy (valgfritt, men sterkt anbefalt)

For at få tilgang til GitLab-verktøy (sammenslåingsforespørsler, problemer, pipelines, CI/CD, etc.):

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "plugin": ["@gitlab/opencode-gitlab-plugin"]
}
```

Denne plugin-en gir omfattende GitLab-repository-administrasjonsfunksjoner, inkludert MR-anmeldelser, problemsporing, pipeline-overvåking og mer.

---

### GitHub Copilot

Slik bruger du GitHub Copilot-abonnementet med OpenCode:

:::note
Nogle modeller kan trenge en [Pro+
abonnement](https://github.com/features/copilot/plans) at bruge.

Nogle modeller må aktiveres manuelt i [GitHub Copilot-innstillingene](https://docs.github.com/en/copilot/how-tos/use-ai-models/configure-access-to-ai-models#setup-for-individual-use).
:::

1. Kjør kommandoen `/connect` og søk etter GitHub Copilot.

   ```txt
   /connect
   ```

2. Naviger til [github.com/login/device](https://github.com/login/device) og skriv inn koden.

   ```txt
   ┌ Login with GitHub Copilot
   │
   │ https://github.com/login/device
   │
   │ Enter code: 8F43-6FCF
   │
   └ Waiting for authorization...
   ```

3. Kjør nå kommandoen `/models` for at velge modellen du ønsker.

   ```txt
   /models
   ```

---

### Google Vertex AI

Slik bruger du Google Vertex AI med OpenCode:

1. Gå over til **Model Garden** i Google Cloud Console og tjek
   modeller tilgængelig i din region.

   :::note
   Du må ha et Google Cloud-prosjekt med Vertex AI API aktivert.
   :::

2. Angi de nødvendige miljøvariablene:
   - `GOOGLE_CLOUD_PROJECT`: Google Cloud-prosjektet ditt ID
   - `VERTEX_LOCATION` (valgfritt): Regionen for verteks AI (standard til `global`)
   - Autentisering (velg en):
     - `GOOGLE_APPLICATION_CREDENTIALS`: Bane til tjenestekontoen JSON nøkkelfil
     - Autentiser med gcloud CLI: `gcloud auth application-default login`

   Sett dem mens du kjører opencode.

   ```bash
   GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json GOOGLE_CLOUD_PROJECT=your-project-id opencode
   ```

   Eller tilføj dem til på bash-profilen din.

   ```bash title="~/.bash_profile"
   export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
   export GOOGLE_CLOUD_PROJECT=your-project-id
   export VERTEX_LOCATION=global
   ```

:::tip
`global`-regionen forbedrer tilgængeligheten og reduserer feil uten ekstra kostnad. Brug regionale endepunkter (f.eks. `us-central1`) for krav til dataopphold. [Finn ut mer](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-partner-models#regional_and_global_endpoints)
:::

3. Kjør kommandoen `/models` for at velge modellen du ønsker.

   ```txt
   /models
   ```

---

### Groq

1. Gå over til [Groq-konsollen](https://console.groq.com/), klikk på **Create API Key**, og kopier nøkkelen.

2. Kjør kommandoen `/connect` og søk etter Groq.

   ```txt
   /connect
   ```

3. Skriv inn API-nøgleen for leverandøren.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør kommandoen `/models` for at velge den du ønsker.

   ```txt
   /models
   ```

---

### Klemmer ansikt

[Hugging Face Inference Providers](https://huggingface.co/docs/inference-providers) gir tilgang til åpne modeller som støttes av 17+ leverandører.

1. Gå over til [Hugging Face-innstillinger](https://huggingface.co/settings/tokens/new?ownUserPermissions=inference.serverless.write&tokenType=fineGrained) for at oprete et token med tillatelse til at ringe til inferensleverandører.

2. Kjør kommandoen `/connect` og søk etter **Hugging Face**.

   ```txt
   /connect
   ```

3. Skriv inn ditt Hugging Face-token.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør kommandoen `/models` for at velge en modell som _Kimi-K2-Instruct_ eller _GLM-4.6_.

   ```txt
   /models
   ```

---

### Helikon

[Helicone](https://helicone.ai) er en LLM observerbarhetsplattform som gir logging, overvåking og analyser for AI-applikasjonene dine. Helicone AI Gateway ruter forespørslene dine til riktig leverandør automatisk basert på modellen.

1. Gå over til [Helicone](https://helicone.ai), opret en konto og generer en API-nøgle fra dashbordet.

2. Kjør kommandoen `/connect` og søk etter **Helicone**.

   ```txt
   /connect
   ```

3. Skriv inn Helicone API-nøgleen.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør kommandoen `/models` for at velge en modell.

   ```txt
   /models
   ```

For flere leverandører og avanserte funksjoner som bufring og hastighetsbegrensning, tjek [Helicone-dokumentasjonen](https://docs.helicone.ai).

#### Valgfrie konfigurasjoner

I tilfelle du ser en funksjon eller modell fra Helicone som ikke konfigureres automatisk gjennom opencode, kan du alltid konfigurere den selv.

Her er [Helicone's Model Directory](https://helicone.ai/models), du trenger denne for at hente ID-ene til modellene du vil tilføje til.

```jsonc title="~/.config/opencode/opencode.jsonc"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "helicone": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Helicone",
      "options": {
        "baseURL": "https://ai-gateway.helicone.ai",
      },
      "models": {
        "gpt-4o": {
          // Model ID (from Helicone's model directory page)
          "name": "GPT-4o", // Your own custom name for the model
        },
        "claude-sonnet-4-20250514": {
          "name": "Claude Sonnet 4",
        },
      },
    },
  },
}
```

#### Egendefinerte topptekster

Helicone støtter tilpassede overskrifter for funksjoner som bufring, brugersporing og sessionadministrasjon. Legg dem til leverandørkonfigurasjonen din ved at bruge `options.headers`:

```jsonc title="~/.config/opencode/opencode.jsonc"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "helicone": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Helicone",
      "options": {
        "baseURL": "https://ai-gateway.helicone.ai",
        "headers": {
          "Helicone-Cache-Enabled": "true",
          "Helicone-User-Id": "opencode",
        },
      },
    },
  },
}
```

##### Sesjonssporing

Helicones [Sessions](https://docs.helicone.ai/features/sessions) funksjon lar deg gruppere relaterte LLM forespørsler sammen. Brug [opencode-helicone-session](https://github.com/H2Shami/opencode-helicone-session) plugin for automatisk at logge hver OpenCode-samtale som en session i Helicone.

```bash
npm install -g opencode-helicone-session
```

Legg den til i konfigurasjonen din.

```json title="opencode.json"
{
  "plugin": ["opencode-helicone-session"]
}
```

Programtiltilføjet injiserer `Helicone-Session-Id` og `Helicone-Session-Name` overskrifter i forespørslene dine. På Helicones Sessions-side vil du se hver OpenCode-samtale oppført som en separat session.

##### Vanlige helikonhoder

| Overskrift                 | Beskrivelse                                                                |
| -------------------------- | -------------------------------------------------------------------------- |
| `Helicone-Cache-Enabled`   | Aktiver responsbufring (`true`/`false`)                                    |
| `Helicone-User-Id`         | Spor beregninger etter bruger                                              |
| `Helicone-Property-[Name]` | Legg til egendefinerte egenskaper (f.eks. `Helicone-Property-Environment`) |
| `Helicone-Prompt-Id`       | Knytt forespørsler til spørsmålsversioner                                  |

Se [Helicone Header Directory](https://docs.helicone.ai/helicone-headers/header-directory) for alle tilgængelige overskrifter.

---

### lama.cpp

Du kan konfigurere opencode for at bruge lokale modeller gjennom [llama.cpps](https://github.com/ggml-org/llama.cpp) lama-server-verktøy

```json title="opencode.json" "llama.cpp" {5, 6, 8, 10-15}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "llama.cpp": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "llama-server (local)",
      "options": {
        "baseURL": "http://127.0.0.1:8080/v1"
      },
      "models": {
        "qwen3-coder:a3b": {
          "name": "Qwen3-Coder: a3b-30b (local)",
          "limit": {
            "context": 128000,
            "output": 65536
          }
        }
      }
    }
  }
}
```

I dette eksemplet:

- `llama.cpp` er den tilpassede leverandøren ID. Dette kan være hvilken som helst streng du vil.
- `npm` spesifiserer pakken som skal bruges for denne leverandøren. Her bruges `@ai-sdk/openai-compatible` for enhver OpenAI-kompatibel API.
- `name` er visningsnavnet for leverandøren i UI.
- `options.baseURL` er endepunktet for den lokale serveren.
- `models` er et kart over modell-ID-er til deres konfigurasjoner. Modellnavnet vil vises i modellvalglisten.

---

### IO.NET

IO.NET tilbyr 17 modeller optimert for ulike brugstilfeller:

1. Gå over til [IO.NET-konsollen](https://ai.io.net/), opret en konto og generer en API-nøgle.

2. Kjør kommandoen `/connect` og søk etter **IO.NET**.

   ```txt
   /connect
   ```

3. Skriv inn nøkkelen IO.NET API.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør kommandoen `/models` for at velge en modell.

   ```txt
   /models
   ```

---

### LM Studio

Du kan konfigurere opencode for at bruge lokale modeller gjennom LM Studio.

```json title="opencode.json" "lmstudio" {5, 6, 8, 10-14}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "lmstudio": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "LM Studio (local)",
      "options": {
        "baseURL": "http://127.0.0.1:1234/v1"
      },
      "models": {
        "google/gemma-3n-e4b": {
          "name": "Gemma 3n-e4b (local)"
        }
      }
    }
  }
}
```

I dette eksemplet:

- `lmstudio` er den tilpassede leverandøren ID. Dette kan være hvilken som helst streng du vil.
- `npm` spesifiserer pakken som skal bruges for denne leverandøren. Her bruges `@ai-sdk/openai-compatible` for enhver OpenAI-kompatibel API.
- `name` er visningsnavnet for leverandøren i UI.
- `options.baseURL` er endepunktet for den lokale serveren.
- `models` er et kart over modell-ID-er til deres konfigurasjoner. Modellnavnet vil vises i modellvalglisten.

---

### Måneskudd AI

Slik bruger du Kimi K2 fra Moonshot AI:

1. Gå over til [Moonshot AI-konsollen](https://platform.moonshot.ai/console), opret en konto og klikk på **Opret API-nøgle**.

2. Kjør kommandoen `/connect` og søk etter **Moonshot AI**.

   ```txt
   /connect
   ```

3. Skriv inn Moonshot API-nøgleen.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør kommandoen `/models` for at velge _Kimi K2_.

   ```txt
   /models
   ```

---

### MiniMax

1. Gå over til [MiniMax API-konsollen](https://platform.minimax.io/login), opret en konto og generer en API-nøgle.

2. Kjør kommandoen `/connect` og søk etter **MiniMax**.

   ```txt
   /connect
   ```

3. Skriv inn MiniMax API-nøgleen.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør kommandoen `/models` for at velge en modell som _M2.1_.

   ```txt
   /models
   ```

---

### Nebius Token Factory

1. Gå over til [Nebius Token Factory-konsollen](https://tokenfactory.nebius.com/), opret en konto og klikk på **Legg til nøkkel**.

2. Kjør kommandoen `/connect` og søk etter **Nebius Token Factory**.

   ```txt
   /connect
   ```

3. Skriv inn Nebius Token Factory API-nøgleen.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør kommandoen `/models` for at velge en modell som _Kimi K2 Instruct_.

   ```txt
   /models
   ```

---

### Ollama

Du kan konfigurere opencode for at bruge lokale modeller gjennom Ollama.

:::tip
Ollama kan automatisk konfigurere seg selv for OpenCode. Se [Ollama-integrasjonsdokumentene](https://docs.ollama.com/integrations/opencode) for detaljer.
:::

```json title="opencode.json" "ollama" {5, 6, 8, 10-14}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "ollama": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Ollama (local)",
      "options": {
        "baseURL": "http://localhost:11434/v1"
      },
      "models": {
        "llama2": {
          "name": "Llama 2"
        }
      }
    }
  }
}
```

I dette eksemplet:

- `ollama` er den tilpassede leverandøren ID. Dette kan være hvilken som helst streng du vil.
- `npm` spesifiserer pakken som skal bruges for denne leverandøren. Her bruges `@ai-sdk/openai-compatible` for enhver OpenAI-kompatibel API.
- `name` er visningsnavnet for leverandøren i UI.
- `options.baseURL` er endepunktet for den lokale serveren.
- `models` er et kart over modell-ID-er til deres konfigurasjoner. Modellnavnet vil vises i modellvalglisten.

:::tip
Hvis verktøykall ikke fungerer, prøv at øke `num_ctx` i Ollama. Start rundt 16k - 32k.
:::

---

### Ollama Cloud

Slik bruger du Ollama Cloud med OpenCode:

1. Gå over til [https://ollama.com/](https://ollama.com/) og logg på eller opret en konto.

2. Naviger til **Innstillinger** > **Nøkler** og klikk på **Legg til API-nøgle** for at generere en ny API-nøgle.

3. Kopier API-nøgleen for brug i OpenCode.

4. Kjør kommandoen `/connect` og søk etter **Ollama Cloud**.

   ```txt
   /connect
   ```

5. Skriv inn din Ollama Cloud API-nøgle.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

6. **Viktig**: Før du bruger skymodeller i OpenCode, må du hente modellinformasjonen lokalt:

   ```bash
   ollama pull gpt-oss:20b-cloud
   ```

7. Kjør kommandoen `/models` for at velge din Ollama Cloud-modell.

   ```txt
   /models
   ```

---

### OpenAI

Vi anbefaler at du registrerer deg for [ChatGPT Plus eller Pro](https://chatgpt.com/pricing).

1. Når du har registrert deg, kjør kommandoen `/connect` og velg OpenAI.

   ```txt
   /connect
   ```

2. Her kan du velge alternativet **ChatGPT Plus/Pro** og det åpner nettleseren din
   og ber deg om at autentisere.

   ```txt
   ┌ Select auth method
   │
   │ ChatGPT Plus/Pro
   │ Manually enter API Key
   └
   ```

3. Nå skal alle OpenAI-modellene være tilgængelige når du bruger kommandoen `/models`.

   ```txt
   /models
   ```

##### Bruge API-tastene

Hvis du allerede har en API-nøgle, kan du velge **Angi API-nøgle manuelt** og lime den inn i terminalen.

---

### OpenCode Zen

OpenCode Zen er en liste over testede og verifiserte modeller levert av OpenCode-teamet. [Finn ut mer](/docs/zen).

1. Logg på **<a href={console}>OpenCode Zen</a>** og klikk på **Create API Key**.

2. Kjør kommandoen `/connect` og søk etter **OpenCode Zen**.

   ```txt
   /connect
   ```

3. Skriv inn OpenCode API-nøgleen.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør kommandoen `/models` for at velge en modell som _Qwen 3 Coder 480B_.

   ```txt
   /models
   ```

---

### OpenRouter

1. Gå over til [OpenRouter-dashbordet](https://openrouter.ai/settings/keys), klikk på **Create API Key**, og kopier nøkkelen.

2. Kjør kommandoen `/connect` og søk etter OpenRouter.

   ```txt
   /connect
   ```

3. Skriv inn API-nøgleen for leverandøren.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Mange OpenRouter-modeller er forhåndslastet som standard, kjør kommandoen `/models` for at velge den du ønsker.

   ```txt
   /models
   ```

   Du kan også tilføje til flere modeller gjennom opencode-konfigurasjonen.

   ```json title="opencode.json" {6}
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "openrouter": {
         "models": {
           "somecoolnewmodel": {}
         }
       }
     }
   }
   ```

5. Du kan også tilpasse dem gjennom opencode-konfigurasjonen. Her er et eksempel på at spesifisere en leverandør

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "openrouter": {
         "models": {
           "moonshotai/kimi-k2": {
             "options": {
               "provider": {
                 "order": ["baseten"],
                 "allow_fallbacks": false
               }
             }
           }
         }
       }
     }
   }
   ```

---

### SAP AI Kjerne

SAP AI Core gir tilgang til 40+ modeller fra OpenAI, Anthropic, Google, Amazon, Meta, Mistral og AI21 gjennom en enhetlig plattform.

1. Gå til din [SAP BTP Cockpit](https://account.hana.ondemand.com/), naviger til din SAP AI kjernetjenesteforekomst, og lag en tjenestenøkkel.

   :::tip
   Tjenestenøkkelen er et JSON-objekt som inneholder `clientid`, `clientsecret`, `url` og `serviceurls.AI_API_URL`. Du finner AI kjerneforekomsten din under **Tjenester** > **Forekomster og abonnementer** i BTP cockpiten.
   :::

2. Kjør kommandoen `/connect` og søk etter **SAP AI Core**.

   ```txt
   /connect
   ```

3. Skriv inn tjenestenøkkelen JSON.

   ```txt
   ┌ Service key
   │
   │
   └ enter
   ```

   Eller angi miljøvariabelen `AICORE_SERVICE_KEY`:

   ```bash
   AICORE_SERVICE_KEY='{"clientid":"...","clientsecret":"...","url":"...","serviceurls":{"AI_API_URL":"..."}}' opencode
   ```

   Eller tilføj den til bash-profilen din:

   ```bash title="~/.bash_profile"
   export AICORE_SERVICE_KEY='{"clientid":"...","clientsecret":"...","url":"...","serviceurls":{"AI_API_URL":"..."}}'
   ```

4. Angi eventuelt distribusjon ID og ressursgruppe:

   ```bash
   AICORE_DEPLOYMENT_ID=your-deployment-id AICORE_RESOURCE_GROUP=your-resource-group opencode
   ```

   :::note
   Disse innstillingene er valgfrie og bør konfigureres i henhold til SAP AI kjerneoppsettet.
   :::

5. Kjør kommandoen `/models` for at velge fra 40+ tilgængelige modeller.

   ```txt
   /models
   ```

---

### OVHcloud AI endepunkter

1. Gå over til [OVHcloud-panelet](https://ovh.com/manager). Naviger til `Public Cloud`-delen, `AI & Machine Learning` > `AI Endpoints` og i `API Keys`-fanen klikker du på **Opret en ny API-nøgle**.

2. Kjør kommandoen `/connect` og søk etter **OVHcloud AI endepunkter**.

   ```txt
   /connect
   ```

3. Skriv inn OVHcloud AI Endpoints API-nøgle.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør kommandoen `/models` for at velge en modell som _gpt-oss-120b_.

   ```txt
   /models
   ```

---

### Scaleway

Slik bruger du [Scaleway Generative APIs](https://www.scaleway.com/en/docs/generative-apis/) med OpenCode:

1. Gå over til [Scaleway Console IAM innstillinger](https://console.scaleway.com/iam/api-keys) for at generere en ny API-nøgle.

2. Kjør kommandoen `/connect` og søk etter **Scaleway**.

   ```txt
   /connect
   ```

3. Skriv inn Scaleway API-nøgleen.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør kommandoen `/models` for at velge en modell som _devstral-2-123b-instruct-2512_ eller _gpt-oss-120b_.

   ```txt
   /models
   ```

---

### Sammen AI

1. Gå over til [Together AI-konsollen](https://api.together.ai), opret en konto og klikk på **Legg til nøkkel**.

2. Kjør kommandoen `/connect` og søk etter **Together AI**.

   ```txt
   /connect
   ```

3. Skriv inn Together AI API-nøgleen.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør kommandoen `/models` for at velge en modell som _Kimi K2 Instruct_.

   ```txt
   /models
   ```

---

### Venezia AI

1. Gå over til [Venezia AI-konsollen](https://venice.ai), opret en konto og generer en API-nøgle.

2. Kjør kommandoen `/connect` og søk etter **Venezia AI**.

   ```txt
   /connect
   ```

3. Skriv inn Venezia AI API-nøgleen.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør kommandoen `/models` for at velge en modell som _Llama 3.3 70B_.

   ```txt
   /models
   ```

---

### Vercel AI Gateway

Vercel AI Gateway lar deg få tilgang til modeller fra OpenAI, Anthropic, Google, xAI og mer gjennom et enhetlig endepunkt. Modeller tilbys til listepris uten påslag.

1. Gå over til [Vercel dashboard](https://vercel.com/), naviger til fanen **AI Gateway**, og klikk på **API nøkler** for at oprete en ny API-nøgle.

2. Kjør kommandoen `/connect` og søk etter **Vercel AI Gateway**.

   ```txt
   /connect
   ```

3. Skriv inn Vercel AI Gateway API-nøgle.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør kommandoen `/models` for at velge en modell.

   ```txt
   /models
   ```

Du kan også tilpasse modeller gjennom opencode-konfigurasjonen. Her er et eksempel på spesifisering av leverandørrutingsrekkefølge.

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "vercel": {
      "models": {
        "anthropic/claude-sonnet-4": {
          "options": {
            "order": ["anthropic", "vertex"]
          }
        }
      }
    }
  }
}
```

Nogle nyttige rutealternativer:

| Alternativ          | Beskrivelse                                                   |
| ------------------- | ------------------------------------------------------------- |
| `order`             | Providersekvens for at prøve                                  |
| `only`              | Begrens til spesifikke leverandører                           |
| `zeroDataRetention` | Brug kun leverandører med null retningslinjer for datalagring |

---

### xAI

1. Gå over til [xAI-konsollen](https://console.x.ai/), opret en konto og generer en API-nøgle.

2. Kjør kommandoen `/connect` og søk etter **xAI**.

   ```txt
   /connect
   ```

3. Skriv inn xAI API-nøgleen.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør kommandoen `/models` for at velge en modell som _Grok Beta_.

   ```txt
   /models
   ```

---

### Z.AI

1. Gå over til [Z.AI API-konsollen](https://z.ai/manage-apikey/apikey-list), opret en konto og klikk på **Opret en ny API-nøgle**.

2. Kjør kommandoen `/connect` og søk etter **Z.AI**.

   ```txt
   /connect
   ```

   Hvis du abonnerer på **GLM Coding Plan**, velg **Z.AI Coding Plan**.

3. Skriv inn Z.AI API-nøgleen.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Kjør kommandoen `/models` for at velge en modell som _GLM-4.7_.

   ```txt
   /models
   ```

---

### ZenMux

1. Gå over til [ZenMux-dashbordet](https://zenmux.ai/settings/keys), klikk på **Create API Key**, og kopier nøkkelen.

2. Kjør kommandoen `/connect` og søk etter ZenMux.

   ```txt
   /connect
   ```

3. Skriv inn API-nøgleen for leverandøren.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Mange ZenMux-modeller er forhåndslastet som standard, kjør kommandoen `/models` for at velge den du ønsker.

   ```txt
   /models
   ```

   Du kan også tilføje til flere modeller gjennom opencode-konfigurasjonen.

   ```json title="opencode.json" {6}
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "zenmux": {
         "models": {
           "somecoolnewmodel": {}
         }
       }
     }
   }
   ```

---

## Egendefinert leverandør

Slik tilføjer du til en **OpenAI-kompatibel**-leverandør som ikke er oppført i `/connect`-kommandoen:

:::tip
Du kan bruge hvilken som helst OpenAI-kompatibel leverandør med opencode. De fleste moderne AI-leverandører tilbyr OpenAI-kompatible APIer.
:::

1. Kjør kommandoen `/connect` og rull ned til **Annet**.

   ```bash
   $ /connect

   ┌  Add credential
   │
   ◆  Select provider
   │  ...
   │  ● Other
   └
   ```

2. Skriv inn en unik ID for leverandøren.

   ```bash
   $ /connect

   ┌  Add credential
   │
   ◇  Enter provider id
   │  myprovider
   └
   ```

   :::note
   Velg en minneverdig ID, du vil bruge denne i konfigurasjonsfilen din.
   :::

3. Skriv inn API-nøgleen for leverandøren.

   ```bash
   $ /connect

   ┌  Add credential
   │
   ▲  This only stores a credential for myprovider - you will need to configure it in opencode.json, check the docs for examples.
   │
   ◇  Enter your API key
   │  sk-...
   └
   ```

4. Opret eller opdater `opencode.json`-filen i prosjektkatalogen:

   ```json title="opencode.json" ""myprovider"" {5-15}
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "myprovider": {
         "npm": "@ai-sdk/openai-compatible",
         "name": "My AI ProviderDisplay Name",
         "options": {
           "baseURL": "https://api.myprovider.com/v1"
         },
         "models": {
           "my-model-name": {
             "name": "My Model Display Name"
           }
         }
       }
     }
   }
   ```

   Her er konfigurasjonsalternativene:
   - **npm**: AI SDK pakke at bruge, `@ai-sdk/openai-compatible` for OpenAI-kompatible leverandører
   - **navn**: Visningsnavn i UI.
   - **modeller**: Tilgjengelige modeller.
   - **options.baseURL**: API endepunkt URL.
   - **options.apiKey**: Angi API-nøgleen hvis du ikke bruger auth.
   - **options.headers**: Angi egendefinerte overskrifter.

   Mer om de avanserte alternativene i eksemplet nedenfor.

5. Kjør kommandoen `/models` og din egendefinerte leverandør og modeller vil vises i utvalgslisten.

---

##### Eksempel

Her er et eksempel på innstilling av alternativene `apiKey`, `headers` og modell `limit`.

```json title="opencode.json" {9,11,17-20}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "myprovider": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "My AI ProviderDisplay Name",
      "options": {
        "baseURL": "https://api.myprovider.com/v1",
        "apiKey": "{env:ANTHROPIC_API_KEY}",
        "headers": {
          "Authorization": "Bearer custom-token"
        }
      },
      "models": {
        "my-model-name": {
          "name": "My Model Display Name",
          "limit": {
            "context": 200000,
            "output": 65536
          }
        }
      }
    }
  }
}
```

Konfigurasjonsdetaljer:

- **apiKey**: Angi med `env` variabel syntaks, [finn ut mer](/docs/config#env-vars).
- **overskrifter**: Egendefinerte overskrifter sendt med hver forespørsel.
- **limit.context**: Maksimalt inndatatoken som modellen godtar.
- **limit.output**: Maksimalt antall tokens modellen kan generere.

`limit`-feltene lar OpenCode forstå hvor mye kontekst du har igjen. Standardleverandører henter disse automatisk fra models.dev.

---

## Feilsøking

Hvis du har problemer med at konfigurere en leverandør, tjek følgende:

1. **Tjek autentiseringsoppsettet**: Kjør `opencode auth list` for at se om legitimasjonen
   for leverandøren tilføjes til konfigurasjonen din.

   Dette gjelder ikke leverandører som Amazon Bedrock, som er avhengige av miljøvariabler for godkjenning.

2. For tilpassede leverandører, tjek opencode-konfigurasjonen og:
   - Sørg for at leverandøren ID som bruges i `/connect`-kommandoen samsvarer med ID i opencode-konfigurasjonen.
     – Den riktige npm-pakken bruges for leverandøren. Brug for eksempel `@ai-sdk/cerebras` for Cerebras. Og for alle andre OpenAI-kompatible leverandører, brug `@ai-sdk/openai-compatible`.
   - Kontroller at riktig API-endepunkt er brugt i `options.baseURL`-feltet.
